from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch
import re
import json
from pathlib import Path


def load_model_gemma(model_path):
    model_path = Path(model_path).resolve()
    base_model_id = "recoilme/recoilme-gemma-2-9B-v0.4"

    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    tokenizer = AutoTokenizer.from_pretrained(base_model_id, local_files_only=True)

    # base model ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=quant_config,
        device_map="auto",
        attn_implementation="eager",
        local_files_only=True
    )

    # PEFT ì–‘ìí™” ëª¨ë¸ìš© ì¤€ë¹„
    base_model = prepare_model_for_kbit_training(base_model)

    # LoRA êµ¬ì¡° ì •ì˜
    peft_config = LoraConfig(
        r=16,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(base_model, peft_config)

    # í•™ìŠµëœ adapter ë¡œë“œ
    model.load_adapter(str(model_path), adapter_name="default")

    model = model.half()
    model.eval()
    model.print_trainable_parameters()
    return tokenizer, model

def load_model_seokdong(model_path):
    model_path = Path(model_path).resolve()
    base_model_id = "SEOKDONG/llama3.1_korean_v1.1_sft_by_aidx"  # ë˜ëŠ” ë‹¤ë¥¸ LLaMA3 ëª¨ë¸

    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    tokenizer = AutoTokenizer.from_pretrained(base_model_id, local_files_only=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=quant_config,
        device_map="auto",
        attn_implementation="eager",
        local_files_only=True
    )

    base_model = prepare_model_for_kbit_training(base_model)

    peft_config = LoraConfig(
        r=16,
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(base_model, peft_config)
    model.load_adapter(str(model_path), adapter_name="default")

    model = model.half()
    model.eval()
    model.print_trainable_parameters()
    return tokenizer, model

def load_model_aidx(model_path):
    model_path = Path(model_path).resolve()
    base_model_id = "AIDX-ktds/ktdsbaseLM-v0.13-onbased-llama3.1"

    # 1. QLoRA ì–‘ìí™” ì„¤ì •
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    # 2. Tokenizer ë¡œë“œ ë° ì„¤ì •
    tokenizer = AutoTokenizer.from_pretrained(base_model_id, local_files_only=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # 3. Base ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=quant_config,
        device_map="auto",
        attn_implementation="eager",
        local_files_only=True
    )

    # 4. QLoRA í•™ìŠµ ì¤€ë¹„
    base_model = prepare_model_for_kbit_training(base_model)

    # 5. LoRA ì„¤ì • (LLaMA3 ì „ìš©)
    peft_config = LoraConfig(
        r=16,
        lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # 6. LoRA ì–´ëŒ‘í„° ì ìš©
    model = get_peft_model(base_model, peft_config)
    model.load_adapter(str(model_path), adapter_name="default")

    # 7. í‰ê°€ëª¨ë“œ ì „í™˜ ë° ë°˜ì •ë°€ë„ ì„¤ì •
    model = model.half()
    model.eval()
    model.print_trainable_parameters()

    return tokenizer, model


# ë“±ê¸‰ê³¼ ì´ìœ  íŒŒì‹± í•¨ìˆ˜
def parse_grade_and_reason(text):
    match = re.search(r"(1ê¸‰|2ê¸‰|3ê¸‰)", text)
    if match:
        grade = match.group(1)
        reason = text[match.end():].strip(" .:\n")
        reason = re.sub(r'^[\s,.:;\-]+', '', reason)
        reason = re.sub(r'[\s,.:;"\'\-\n]+$', '', reason)
    else:
        grade = "ë¯¸ìƒ"
        reason = text.strip()
    return grade, reason

# í”„ë¡¬í”„íŠ¸ ì •ì˜ í…œí”Œë¦¿
base_prompt = """
ë‹¤ìŒì€ ê¸°ìˆ  ë¶„ì•¼ì˜ ë³´ì•ˆë“±ê¸‰ ë¶„ë¥˜ ê¸°ì¤€ì…ë‹ˆë‹¤:

1ê¸‰: ê¸°ìˆ ì •ë³´(ì—°êµ¬, ë¶€í’ˆ, ì„¤ê³„ë„, ì¥ë¹„, ë””ìì¸ ìš”ì†Œ), ê°œì¸ì •ë³´, ê±°ë˜ ë‚©í’ˆ ë¦¬ìŠ¤íŠ¸(ì˜ˆ: ì œì¡° ê¸°ìˆ  ëª…ì‹œ ë“±), íŠ¹ì • ë‹¨ì–´(ê¸°ë°€ìœ ì§€, ë³´ì•ˆê´€ë¦¬ ë“±)ê°€ í¬í•¨ëœ ë¯¼ê°í•œ ë¬¸ì„œ

2ê¸‰: ì¬ì •í˜„í™©, ê°ì‚¬, ê²°ì‚°, ì¸ì‚¬í‰ê°€, ì—…ë¶€ë³´ê³ ì„œ, ì§•ê³„ë¬¸ì„œì™€ ê°™ì€ ë‚´ë¶€ ë³´ê³ ì„œ  

3ê¸‰: 1ê¸‰,2ê¸‰ ìš”ì†Œê°€ í•˜ë‚˜ë„ ì—†ëŠ” ì¼ë°˜ì ì¸ ë¬¸ì„œ(ì˜¤í”ˆì†ŒìŠ¤, ê³µê°œ ì •ë³´ ë“±)

---

â€» íŒë‹¨ ì›ì¹™:

- í•˜ë‚˜ì˜ ë¬¸ì„œì— ì—¬ëŸ¬ ë“±ê¸‰ì˜ ì •ë³´ê°€ ì„ì—¬ ìˆì„ ê²½ìš°, ê°€ì¥ ë†’ì€ ë“±ê¸‰ì„ ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ ë³´ì•ˆë“±ê¸‰ì„ íŒë‹¨í•©ë‹ˆë‹¤.
- 1ê¸‰ ìš”ì†Œê°€ ë‹¨ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ í•´ë‹¹ ë¬¸ì„œëŠ” ë°˜ë“œì‹œ 1ê¸‰ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
- 1ê¸‰ì´ ì—†ê³  2ê¸‰ê³¼ 3ê¸‰ ìš”ì†Œê°€ í•¨ê»˜ í¬í•¨ëœ ê²½ìš°ì—ëŠ” 2ê¸‰ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
- ë‹¨ì¼ ë“±ê¸‰ ì •ë³´ë§Œ í¬í•¨ëœ ê²½ìš°ì—ëŠ” í•´ë‹¹ ë“±ê¸‰ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
- ì•„ë˜ì— ì œì‹œëœ "ë¬¸ì„œ ë‚´ í¬í•¨ ì´ë¯¸ì§€ ë‚´ìš©" ì™¸ì—ëŠ” ë³„ë„ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©°, í•´ë‹¹ í•­ëª©ë§Œ ì‹œê° ì •ë³´ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

---

ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë³´ì•ˆë“±ê¸‰ì„ 1ê¸‰, 2ê¸‰, 3ê¸‰ ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨í•˜ê³ , ê·¸ ì´ìœ ë„ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”.

ë¬¸ì„œ:
\"\"\"{doc_text}\"\"\"
ë¬¸ì„œ ë‚´ í¬í•¨ ì´ë¯¸ì§€ ë‚´ìš© (í•´ë‹¹ í˜ì´ì§€ì˜ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ì‹œê° ì •ë³´ì…ë‹ˆë‹¤):
\"\"\"{img_text}\"\"\"

ë³´ì•ˆë“±ê¸‰ ë° ì´ìœ :
"""

# ë©”ì¸ ì¸í¼ëŸ°ìŠ¤ í•¨ìˆ˜
def run_inference(input_path="output/document.jsonl", output_path="output/output_results.jsonl", model_path="module/llm/fine_tune/checkpoints/finetuned_gemma_qlora"):
    if model_path == "module/llm/fine_tune/checkpoints/finetuned_gemma_qlora":
        tokenizer, model = load_model_gemma(model_path)  # ì ¬ë§ˆëª¨ë¸ ì‚¬ìš© - ë””í´íŠ¸
    elif model_path == "module/llm/fine_tune/checkpoints_ktds_llama3/finetuned_ktds_llama3_qlora":
        tokenizer, model = load_model_aidx(model_path)
    #tokenizer, model = load_model_aidx(model_path) # aidxëª¨ë¸ ì‚¬ìš©
    elif model_path == "module/llm/fine_tune/checkpoints_llama3/finetuned_llama3_qlora":
        tokenizer, model = load_model_seokdong(model_path)
        
    #tokenizer, model = load_model_seokdong(model_path) # seokdongëª¨ë¸ ì‚¬ìš©
    else:
        print("âŒ ì˜ëª»ëœ ëª¨ë¸ ê²½ë¡œì…ë‹ˆë‹¤. ì ¬ë§ˆ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        tokenizer, model = load_model_gemma(model_path)

    with open(input_path, "r", encoding="utf-8") as infile, \
         open(output_path, "w", encoding="utf-8") as outfile:

        for line in infile:
            data = json.loads(line)
            doc_text = data.get("text", "").strip()
            page_num = data.get("page", None)

            img_text_raw = data.get("image", "")
            img_text = ", ".join(img_text_raw) if isinstance(img_text_raw, list) else str(img_text_raw)
            img_text = img_text.strip()

            if not doc_text or page_num is None:
                print("âš ï¸ ì…ë ¥ ë°ì´í„°ì— 'text' ë˜ëŠ” 'page'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            prompt = base_prompt.format(doc_text=doc_text, img_text=img_text)

            # 1. Tokenize
            inputs = tokenizer(prompt, return_tensors="pt",truncation=True, max_length=2048)

            # 2. input_idsëŠ” long ìœ ì§€, attention_mask ë“±ì€ float16 ë³€í™˜
            for k in inputs:
                if k == "input_ids":
                    inputs[k] = inputs[k].to(model.device)
                else:
                    inputs[k] = inputs[k].to(model.device).to(torch.float16)
            input_length = inputs["input_ids"].shape[1]

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=300,
                    temperature=0.0,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    do_sample=False
                )

            generated_tokens = outputs[0][input_length:]
            generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

            if "ë¬¸ì„œ:" in generated_text:
                generated_text = generated_text.split("ë¬¸ì„œ:")[0].strip()
            # ê°œí–‰(\n) ê¸°ì¤€ìœ¼ë¡œ ì²« ë¬¸ì¥ë§Œ ì¶”ì¶œ
            generated_text = generated_text.split('\n')[0].strip()
            grade, reason = parse_grade_and_reason(generated_text)

            result = {
                "page": page_num,
                "grade": grade,
                "reason": reason
            }
            outfile.write(json.dumps(result, ensure_ascii=False) + "\n")
            print(f"âœ… {page_num}í˜ì´ì§€ ì™„ë£Œ - {grade}")
    # ğŸ¯ ëª¨ë¸ ì œê±° ë° ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    torch.cuda.empty_cache()    
