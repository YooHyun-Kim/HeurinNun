import torch
import json
import re
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

def load_base_model():
    base_model_id = "beomi/open-llama-2-ko-7b"

    tokenizer = AutoTokenizer.from_pretrained(
        base_model_id,
        trust_remote_code=True,
        local_files_only=False   # ì²˜ìŒ ë‹¤ìš´ë¡œë“œí•  ë•Œë§Œ False
    )

    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        device_map="auto",
        torch_dtype=torch.float16,     # float16 ì‚¬ìš©
        attn_implementation="eager",
        trust_remote_code=True,
        local_files_only=False         # ì²˜ìŒ ë‹¤ìš´ë¡œë“œí•  ë•Œë§Œ False
    )

    model.eval()
    return tokenizer, model

# ğŸ“Œ ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
summarize_prompt = """
ë‹¤ìŒì€ ê° í˜ì´ì§€ë³„ë¡œ ë³´ì•ˆë“±ê¸‰ì„ íŒë‹¨í•œ ì´ìœ  ëª¨ìŒì…ë‹ˆë‹¤. 
ë“±ê¸‰ì„ ê²°ì •í•œ í•µì‹¬ì ì¸ ì´ìœ ë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ì„œ ì‘ì„±í•˜ì„¸ìš”.
ë¶ˆí•„ìš”í•œ ë°˜ë³µì€ í”¼í•˜ê³ , ìš”ì•½ì€ 1~2ë¬¸ì¥ ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ë¶ˆí•„ìš”í•œ ì„¤ëª…, ê¸°ê´€ëª…, ë‚ ì§œ ë“±ì€ ìƒëµí•˜ê³ , í•µì‹¬ ê¸°ìˆ /ì„¤ê³„/êµ¬ì„± ì •ë³´ë§Œ ê°•ì¡°í•´ ì£¼ì„¸ìš”.

ì´ìœ  ëª©ë¡:
\"\"\"{reasons}\"\"\"

ìš”ì•½:
"""

# âœ… ìš”ì•½ ìˆ˜í–‰ í•¨ìˆ˜
def summarize_results(results_path="output/output_results.jsonl"):
    # 1. ëª¨ë¸ ë¡œë“œ
    tokenizer, model = load_base_model()

    # 2. ë“±ê¸‰ ë° ì´ìœ  ìˆ˜ì§‘ (ë“±ê¸‰ë³„ë¡œ ë¶„ë¦¬ ì €ì¥)
    grade_reason_map = {"1ê¸‰": [], "2ê¸‰": [], "3ê¸‰": []}
    with open(results_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            grade = data.get("grade", "").strip()
            reason = data.get("reason", "").strip()
            if grade and reason and grade in grade_reason_map:
                grade_reason_map[grade].append(reason)

    # 3. ìµœì¢… ë“±ê¸‰ ê²°ì • ë° í•´ë‹¹ ì´ìœ  ì„ íƒ
    if grade_reason_map["1ê¸‰"]:
        final_grade = "1ê¸‰"
        selected_reasons = grade_reason_map["1ê¸‰"]
    elif grade_reason_map["2ê¸‰"]:
        final_grade = "2ê¸‰"
        selected_reasons = grade_reason_map["2ê¸‰"]
    else:
        final_grade = "3ê¸‰"
        selected_reasons = grade_reason_map["3ê¸‰"]


    # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = summarize_prompt.format(reasons="\n".join(selected_reasons))

    # 5. ì…ë ¥ í† í¬ë‚˜ì´ì¦ˆ
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
    for k in inputs:
        inputs[k] = inputs[k].to(model.device) if k == "input_ids" else inputs[k].to(model.device).to(torch.float16)
    input_length = inputs["input_ids"].shape[1]

    # 6. ìš”ì•½ ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.2,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2
        )

    generated_tokens = outputs[0][input_length:]
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    # 7. "ìš”ì•½:" ì œê±° ë° ë§ˆë¬´ë¦¬
    if "ìš”ì•½:" in generated_text:
        generated_text = generated_text.split("ìš”ì•½:")[-1].strip()
    summary = generated_text.strip()

    # 8. ê²°ê³¼ ì¶œë ¥
    print(f"âœ… ìµœì¢… ë³´ì•ˆë“±ê¸‰: {final_grade}")
    print(f"ğŸ“ ìš”ì•½ ê²°ê³¼:\n{summary}")

    # 9. ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    torch.cuda.empty_cache()
    gc.collect()

    # # 10. JSON ë°˜í™˜
    # return {
    #     "final_grade": final_grade,
    #     "final_summary": summary
    # }
