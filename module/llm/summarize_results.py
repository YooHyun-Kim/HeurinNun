import torch
import json
import re
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import random
def load_base_model():
    base_model_id = "beomi/open-llama-2-ko-7b"

    tokenizer = AutoTokenizer.from_pretrained(
        base_model_id,
        trust_remote_code=True,
        local_files_only=False   # ì²˜ìŒ ë‹¤ìš´ë¡œë“œí•  ë•Œë§Œ False
    )

    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        device_map="auto",
        torch_dtype=torch.float16,     # float16 ì‚¬ìš©
        attn_implementation="eager",
        trust_remote_code=True,
        local_files_only=False         # ì²˜ìŒ ë‹¤ìš´ë¡œë“œí•  ë•Œë§Œ False
    )

    model.eval()
    return tokenizer, model

# ğŸ“Œ ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
summarize_prompt = """
ë³´ì•ˆë“±ê¸‰: {grade}

ë‹¤ìŒì€ í•´ë‹¹ ë“±ê¸‰ìœ¼ë¡œ íŒë‹¨ëœ ì´ìœ ë“¤ì…ë‹ˆë‹¤.
ì¤‘ë³µë˜ê±°ë‚˜ ìœ ì‚¬í•œ ì´ìœ ëŠ” í•˜ë‚˜ë¡œ ë¬¶ê³ , í•µì‹¬ ê¸°ìˆ /ì„¤ê³„/êµ¬ì„± ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ 2~3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•˜ì‹­ì‹œì˜¤.
ê¸°ê´€ëª…, ë‚ ì§œ, ì¶”ìƒì ì¸ ì„¤ëª…ì€ ì œì™¸í•˜ê³  êµ¬ì²´ì ì¸ ê¸°ìˆ ì  ì´ìœ ë§Œ í¬í•¨í•˜ì‹­ì‹œì˜¤.

ì´ìœ  ëª©ë¡:
\"\"\"{reasons}\"\"\"

ìš”ì•½:
"""

# âœ… ìš”ì•½ ìˆ˜í–‰ í•¨ìˆ˜
def summarize_results(results_path="output/output_results.jsonl"):
    # 1. ëª¨ë¸ ë¡œë“œ
    tokenizer, model = load_base_model()

    # 2. ë“±ê¸‰ ë° ì´ìœ  ìˆ˜ì§‘ (ë“±ê¸‰ë³„ë¡œ ë¶„ë¦¬ ì €ì¥)
    grade_reason_map = {"1ê¸‰": [], "2ê¸‰": [], "3ê¸‰": []}
    with open(results_path, "r", encoding="utf-8") as f:
        for line in f:
            data = json.loads(line)
            grade = data.get("grade", "").strip()
            reason = data.get("reason", "").strip()
            if grade and reason and grade in grade_reason_map:
                grade_reason_map[grade].append(reason)

    # 3. ìµœì¢… ë“±ê¸‰ ê²°ì • ë° í•´ë‹¹ ì´ìœ  ì„ íƒ
    if grade_reason_map["1ê¸‰"]:
        final_grade = "1ê¸‰"
        selected_reasons = grade_reason_map["1ê¸‰"]
    elif grade_reason_map["2ê¸‰"]:
        final_grade = "2ê¸‰"
        selected_reasons = grade_reason_map["2ê¸‰"]
    else:
        final_grade = "3ê¸‰"
        selected_reasons = grade_reason_map["3ê¸‰"]
    selected_reasons = [reason.replace("ì´ìœ :", "").strip() for reason in selected_reasons]
    selected_reasons = list(set(selected_reasons))  # ì¤‘ë³µ ì œê±°
    selected_reasons = random.sample(selected_reasons, min(len(selected_reasons), 10))
    
    print(f"ì„ íƒëœ ì´ìœ  ëª©ë¡: {selected_reasons}")
    # 4. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = summarize_prompt.format(
    grade=final_grade,
    reasons="\n".join(selected_reasons)
)
    # 5. ì…ë ¥ í† í¬ë‚˜ì´ì¦ˆ
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
    for k in inputs:
        inputs[k] = inputs[k].to(model.device) if k == "input_ids" else inputs[k].to(model.device).to(torch.float16)
    input_length = inputs["input_ids"].shape[1]

    # 6. ìš”ì•½ ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.2,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.2
        )

    generated_tokens = outputs[0][input_length:]
    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    # 7. "ìš”ì•½:" ì œê±° ë° ë§ˆë¬´ë¦¬
    if "ìš”ì•½:" in generated_text:
        generated_text = generated_text.split("ìš”ì•½:")[-1].strip()
    summary = generated_text.strip()

    # 8. ê²°ê³¼ ì¶œë ¥
    print(f"âœ… ìµœì¢… ë³´ì•ˆë“±ê¸‰: {final_grade}")
    print(f"ğŸ“ ìš”ì•½ ê²°ê³¼:\n{summary}")

    # 9. ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    torch.cuda.empty_cache()
    gc.collect()

    # # 10. JSON ë°˜í™˜
    # return {
    #     "final_grade": final_grade,
    #     "final_summary": summary
    # }
